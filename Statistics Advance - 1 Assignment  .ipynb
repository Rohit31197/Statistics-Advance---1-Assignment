{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09b175c6-1333-4947-af9a-e81fc85e428d",
   "metadata": {},
   "source": [
    "### Q.1. **Explain the properties of the F-distribution.**\n",
    "\n",
    "The **F-distribution** is a continuous probability distribution that arises in statistical analysis, particularly in the context of **ANOVA** (Analysis of Variance), **regression analysis**, and testing hypotheses involving variances. It is the distribution of the ratio of two independent chi-squared random variables, each divided by their respective degrees of freedom.\n",
    "\n",
    "Here are the key properties of the **F-distribution**:\n",
    "\n",
    " ### 1. **Shape and Nature**\n",
    "   - The **F-distribution** is **positively skewed** and is defined only for **positive values** (i.e., it takes values in the range \\( [0, \\infty) \\)).\n",
    "   - The shape of the distribution depends on the degrees of freedom of the numerator and denominator (which are related to the two chi-squared distributions involved in the ratio). Generally:\n",
    "     - If the degrees of freedom are small, the distribution is more skewed.\n",
    "     - As the degrees of freedom increase, the distribution becomes more symmetric and approaches a normal distribution.\n",
    "\n",
    "### 2. **Degrees of Freedom (df)**\n",
    "   - The **F-distribution** is characterized by two sets of degrees of freedom:\n",
    "     - **Numerator degrees of freedom** (\\( df_1 \\)): This is the degrees of freedom associated with the numerator (the first chi-squared variable).\n",
    "     - **Denominator degrees of freedom** (\\( df_2 \\)): This is the degrees of freedom associated with the denominator (the second chi-squared variable).\n",
    "   - The F-distribution is denoted as \\( F(df_1, df_2) \\), where \\( df_1 \\) and \\( df_2 \\) are the degrees of freedom of the numerator and denominator, respectively.\n",
    "\n",
    "### 3. **Probability Density Function (PDF)**\n",
    "   The probability density function of the **F-distribution** is given by:\n",
    "   \\[\n",
    "   f(x; df_1, df_2) = \\frac{\\sqrt{\\frac{df_1 x}{df_2}}^{df_1}}{\\text{B}\\left(\\frac{df_1}{2}, \\frac{df_2}{2}\\right)} \\cdot \\left(1 + \\frac{df_1}{df_2}x\\right)^{-\\left(\\frac{df_1 + df_2}{2}\\right)}\n",
    "   \\]\n",
    "   where:\n",
    "   - \\( \\text{B} \\) is the **Beta function**.\n",
    "\n",
    "   For practical purposes, most statistical software packages compute the F-distribution and its cumulative distribution function (CDF) directly, rather than using the formula above.\n",
    "\n",
    "### 4. **Mean and Variance**\n",
    "   - **Mean**: The mean of the **F-distribution** is given by:\n",
    "     \\[\n",
    "     \\mu = \\frac{df_2}{df_2 - 2} \\quad \\text{(for \\( df_2 > 2 \\))}\n",
    "     \\]\n",
    "     This shows that the mean exists only if the denominator degrees of freedom \\( df_2 \\) are greater than 2. If \\( df_2 \\leq 2 \\), the mean is undefined.\n",
    "     \n",
    "   - **Variance**: The variance of the **F-distribution** is given by:\n",
    "     \\[\n",
    "     \\sigma^2 = \\frac{2(df_2)^2(df_1 + df_2 - 2)}{df_1(df_2 - 2)^2(df_2 - 4)} \\quad \\text{(for \\( df_2 > 4 \\))}\n",
    "     \\]\n",
    "     The variance also exists only for \\( df_2 > 4 \\).\n",
    "\n",
    "### 5. **Skewness and Kurtosis**\n",
    "   - The **skewness** of the F-distribution is positive (right-skewed), especially for small degrees of freedom.\n",
    "   - The **kurtosis** is typically high, meaning the distribution has heavier tails compared to a normal distribution.\n",
    "\n",
    "### 6. **Use in Hypothesis Testing**\n",
    "   - The **F-distribution** is widely used in **variance analysis** and in testing hypotheses related to the equality of variances between two or more groups. It is commonly seen in:\n",
    "     - **Analysis of Variance (ANOVA)**: To test if the means of several groups are equal by comparing the variances.\n",
    "     - **F-test for regression models**: To test the overall significance of a regression model.\n",
    "   - In these tests, the test statistic follows an F-distribution, and critical values are derived from the F-distribution tables or computed using statistical software.\n",
    "\n",
    "### 7. **Tail Behavior and Critical Values**\n",
    "   - The F-distribution is often used in one-sided tests, as it is non-negative and only has a right tail. Critical values are typically obtained from F-distribution tables or statistical software.\n",
    "   - **Right-tailed** test: For testing whether a ratio of variances exceeds a certain threshold, often used in ANOVA or comparing the variances of two populations.\n",
    "\n",
    "### 8. **Relationship with the Chi-Squared Distribution**\n",
    "   - The **F-distribution** is defined as the ratio of two independent chi-squared random variables:\n",
    "     \\[\n",
    "     F = \\frac{(X_1 / df_1)}{(X_2 / df_2)}\n",
    "     \\]\n",
    "     where \\( X_1 \\sim \\chi^2(df_1) \\) and \\( X_2 \\sim \\chi^2(df_2) \\). Both chi-squared variables are independent, and the F-distribution is their ratio, scaled by their respective degrees of freedom.\n",
    "\n",
    "### Summary of Key Points:\n",
    "- The F-distribution is used primarily in hypothesis tests related to variances.\n",
    "- It is positively skewed and defined for positive values.\n",
    "- It has two parameters: numerator degrees of freedom (\\( df_1 \\)) and denominator degrees of freedom (\\( df_2 \\)).\n",
    "- The mean and variance are dependent on the degrees of freedom and may be undefined for certain parameter values.\n",
    "- It plays a central role in ANOVA, regression analysis, and comparing variances.\n",
    "\n",
    "Understanding these properties helps when interpreting statistical results, particularly in tests that involve comparing variances or assessing the fit of regression models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52119b14-57f9-4a95-b8e9-211224fda9d5",
   "metadata": {},
   "source": [
    "### Q2. **In which types of statistical tests is the F-distribution used, and why is it appropriate for these tests?** \n",
    "\n",
    "The F-distribution is used in statistical tests where we compare variances or examine the ratio of two variances. It’s a key component in:\r\n",
    "\r\n",
    "1. **Analysis of Variance (ANOVA)**: \r\n",
    "   - ANOVA is used to compare the means of three or more groups to determine if at least one of the group means is different from the others. The F-distribution is appropriate here because it helps us understand if the variability between group means is significantly larger than the variability within groups. The F-statistic is a ratio of the between-group variance to the within-group variance.\r\n",
    "   \r\n",
    "2. **Regression Analysis (Overall Significance Testing)**:\r\n",
    "   - In regression, the F-test checks if the overall model is significant by comparing a model with predictor variables to a model without them (often the intercept-only model). The F-statistic tests whether the additional explained variance by the predictors is significant relative to the unexplained variance.\r\n",
    "\r\n",
    "3. **Comparing Two Population Variances**:\r\n",
    "   - The F-distribution is also used when comparing the variances of two populations, often in the form of a variance ratio test. The test statistic here is the ratio of two sample variances, and we use the F-distribution to determine if the observed ratio is significantly different from 1.\r\n",
    "\r\n",
    "The F-distribution is appropriate for these tests because:\r\n",
    "- It describes the ratio of two independent chi-squared distributions, normalized by their respective degrees of freedom.\r\n",
    "- It is positively skewed, which fits the nature of variance ratio tests (variance ratios cannot be negative).\r\n",
    "- The distribution depends on degrees of freedom, allowing it to adjust according to sample size and provide critical values for hypothesis testing in the appropriate context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03472ac6-d5b4-45c3-8122-039b92849423",
   "metadata": {},
   "source": [
    "### Q3. **What are the key assumptions required for conducting an F-test to compare the variances of two populations?**\n",
    "\n",
    "The key assumptions required for conducting an F-test to compare the variances of two populations are:\r\n",
    "\r\n",
    "1. **Independence of Samples**:\r\n",
    "   - The two samples must be independent of each other. This means that the data in one sample should not influence or be related to the data in the other sample.\r\n",
    "\r\n",
    "2. **Normality of the Populations**:\r\n",
    "   - The populations from which the samples are drawn should follow a normal distribution. The F-test is sensitive to deviations from normality, so if this assumption is violated, the results may not be reliable.\r\n",
    "\r\n",
    "3. **Equal Variance Under the Null Hypothesis**:\r\n",
    "   - The F-test assumes that, under the null hypothesis, the variances of the two populations are equal. However, this is only relevant for the interpretation of the test result rather than an assumption of the test itself.\r\n",
    "\r\n",
    "4. **Random Sampling**:\r\n",
    "   - The samples should be randomly selected from the populations to ensure that they are representative and to avoid biases that might distort the test results.\r\n",
    "\r\n",
    "Violating these assumptions, especially normality, can lead to inaccurate conclusions. For non-normal data or small sample sizes, alternative tests, such as the Levene’s test or Bartlett's test, are sometimes preferred because they are more robust to departures from normality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89c0be0-fe9c-4615-b2cf-efed01937b63",
   "metadata": {},
   "source": [
    "### Q4. **What is the purpose of ANOVA, and how does it differ from a t-test?**\n",
    "\n",
    "The purpose of Analysis of Variance (ANOVA) is to determine whether there are statistically significant differences between the means of three or more independent groups. ANOVA evaluates if at least one group mean differs from the others by comparing the variance within each group to the variance between groups. \r\n",
    "\r\n",
    "### Differences between ANOVA and a t-test:\r\n",
    "\r\n",
    "1. **Number of Groups Compared**:\r\n",
    "   - **t-test**: Generally used to compare the means of two groups (independent or paired samples).\r\n",
    "   - **ANOVA**: Designed to compare the means of three or more groups. While ANOVA can technically be used for two groups, a t-test is simpler and more direct for that purpose.\r\n",
    "\r\n",
    "2. **Type of Hypotheses**:\r\n",
    "   - **t-test**: Tests for the difference in means between two groups, specifically whether the difference is significantly different from zero.\r\n",
    "   - **ANOVA**: Tests if there is any significant difference among multiple group means. It doesn’t indicate which groups are different, only that at least one group is different. Post-hoc tests are often needed to identify specific group differences.\r\n",
    "\r\n",
    "3. **Error Rates**:\r\n",
    "   - Conducting multiple t-tests to compare three or more groups increases the likelihood of Type I errors (false positives), as each test compounds the chance of a significant result by chance alone. \r\n",
    "   - **ANOVA** avoids this issue by testing all groups simultaneously, maintaining the overall Type I error rate.\r\n",
    "\r\n",
    "4. **Underlying Calculation**:\r\n",
    "   - **t-test**: Compares the difference in group means relative to the pooled standard error.\r\n",
    "   - **ANOVA**: Calculates the ratio of the variance between group means to the variance within groups (represented by the F-statistic).\r\n",
    "\r\n",
    "### When to Use Each Test:\r\n",
    "- **t-test**: Ideal for studies with two groups or two time points, often where specific pairwise comparisons are of interest.\r\n",
    "- **ANOVA**: Best suited for studies with three or more groups or treatment levels where the focus is on assessing overall mean differences rather than pairwise comparisons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e02d662-cf33-46a9-ae3d-85260424e84e",
   "metadata": {},
   "source": [
    "### Q.5. **Explain when and why you would use a one-way ANOVA instead of multiple t-tests when comparing more than two groups**\n",
    "\n",
    "A one-way ANOVA is preferred over multiple t-tests when comparing the means of three or more groups because:\r\n",
    "\r\n",
    "### 1. **Control of Type I Error Rate**:\r\n",
    "   - **Type I error** occurs when we incorrectly reject a true null hypothesis (a false positive). Conducting multiple t-tests for each pairwise comparison increases the probability of at least one Type I error, as each test has its own alpha level (e.g., 0.05). This cumulative error rate can lead to misleading results.\r\n",
    "   - **ANOVA** addresses this by testing all group means simultaneously under a single test, maintaining the overall Type I error rate at the chosen significance level (e.g., 0.05), regardless of the number of groups.\r\n",
    "\r\n",
    "### 2. **Efficiency and Simplicity**:\r\n",
    "   - Running multiple t-tests for each pairwise comparison among several groups is not only more labor-intensive but also more complex to interpret. For \\( k \\) groups, there would be \\( \\frac{k(k-1)}{2} \\) t-tests, which increases exponentially with the number of groups.\r\n",
    "   - **One-way ANOVA** provides a single test result to assess whether there is a statistically significant difference among any of the group means, simplifying the process.\r\n",
    "\r\n",
    "### 3. **Interpretation of Overall Group Differences**:\r\n",
    "   - **ANOVA** is designed to detect if at least one group mean is different from the others, offering a general view of differences among all groups.\r\n",
    "   - After ANOVA, if the result is significant, **post-hoc tests** (like Tukey’s HSD) can be used to identify specific group differences without increasing the Type I error rate, unlike in multiple t-tests.\r\n",
    "\r\n",
    "In summary, one-way ANOVA is used instead of multiple t-tests for comparing more than two groups to control the Type I error rate, reduce complexity, and allow for a clear interpretation of overall group differences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04500ec-c8a8-4d42-9f7a-b4200f8043be",
   "metadata": {},
   "source": [
    "### Q6. **Explain how variance is partitioned in ANOVA into between-group variance and within-group variance.How does this partitioning contribute to the calculation of the F-statistic?**\n",
    "\n",
    "In ANOVA, variance is partitioned into two main components: **between-group variance** and **within-group variance**. This partitioning is fundamental to understanding how much of the total variation in the data is due to differences between group means (between-group variance) versus variation within each group (within-group variance).\r\n",
    "\r\n",
    "### 1. **Between-Group Variance**:\r\n",
    "   - This component measures the variability in data that can be attributed to differences between the group means.\r\n",
    "   - It reflects how much the group means differ from the overall mean (the mean of all data points, regardless of group).\r\n",
    "   - Mathematically, it is calculated by taking the squared differences between each group mean and the overall mean, then weighting by the number of observations in each group.\r\n",
    "   - A high between-group variance indicates that the groups differ substantially from each other.\r\n",
    "\r\n",
    "### 2. **Within-Group Variance**:\r\n",
    "   - This component measures the variability within each group and reflects the differences among individual observations within the same group.\r\n",
    "   - It’s calculated by examining the differences between each individual observation and its respective group mean, then squaring and summing these differences within each group.\r\n",
    "   - Lower within-group variance indicates that individuals within each group are relatively similar to each other.\r\n",
    "\r\n",
    "### 3. **Total Variance**:\r\n",
    "   - Total variance is the sum of between-group and within-group variance and represents all variation observed in the data.\r\n",
    "   - Mathematically: **Total Sum of Squares (SST) = Between-Group Sum of Squares (SSB) + Within-Group Sum of Squares (SSW)**.\r\n",
    "\r\n",
    "### Contribution to the F-Statistic Calculation:\r\n",
    "The F-statistic in ANOVA is a ratio of between-group variance to within-group variance. This ratio indicates whether the observed differences among group means are larger than what would be expected by chance alone.\r\n",
    "\r\n",
    "   - **F-statistic = Mean Square Between Groups (MSB) / Mean Square Within Groups (MSW)**, where:\r\n",
    "     - **MSB (Mean Square Between)** = Between-group sum of squares (SSB) / degrees of freedom between groups.\r\n",
    "     - **MSW (Mean Square Within)** = Within-group sum of squares (SSW) / degrees of freedom within groups.\r\n",
    "\r\n",
    "   - If the between-group variance is large relative to the within-group variance, the F-statistic will be higher, suggesting that the group means are significantly different from each other. If the F-statistic is near 1, it indicates that the variance between groups is similar to the variance within groups, implying no significant difference among group means.\r\n",
    "\r\n",
    "In summary, partitioning variance into between-group and within-group components allows ANOVA to isolate the effect of group differences, with the F-statistic providing a measure of whether those group differences are statistically significant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340a6be3-8419-4e8c-8c51-7d21b2d595a8",
   "metadata": {},
   "source": [
    "### Q7. **Compare the classical (frequentist) approach to ANOVA with the Bayesian approach. What are the key differences in terms of how they handle uncertainty, parameter estimation, and hypothesis testing?**\n",
    "\n",
    "The classical (frequentist) and Bayesian approaches to ANOVA differ fundamentally in how they handle uncertainty, parameter estimation, and hypothesis testing. Here’s a breakdown of these differences:\r\n",
    "\r\n",
    "### 1. **Handling Uncertainty**:\r\n",
    "   - **Classical (Frequentist) ANOVA**:\r\n",
    "     - Assumes fixed, unknown parameters and relies on probability as the long-term frequency of events.\r\n",
    "     - Uncertainty is expressed through p-values, which indicate the probability of observing the data (or something more extreme) under the null hypothesis.\r\n",
    "     - Inference is based on the sample alone, without incorporating any prior information.\r\n",
    "     \r\n",
    "   - **Bayesian ANOVA**:\r\n",
    "     - Incorporates prior beliefs or information about parameters in the form of prior distributions, which represent uncertainty before observing the data.\r\n",
    "     - The results combine prior beliefs with observed data, producing a **posterior distribution** that reflects updated uncertainty about parameters after accounting for the data.\r\n",
    "     - Provides a distribution of plausible values for each parameter, directly reflecting uncertainty.\r\n",
    "\r\n",
    "### 2. **Parameter Estimation**:\r\n",
    "   - **Classical ANOVA**:\r\n",
    "     - Estimates parameters (e.g., group means, variances) using sample data to produce point estimates (typically means) and confidence intervals.\r\n",
    "     - Confidence intervals provide a range of values that, under repeated sampling, would contain the true parameter a specified percentage of the time.\r\n",
    "     - Uses the F-statistic and p-values to evaluate if observed differences among groups are statistically significant.\r\n",
    "\r\n",
    "   - **Bayesian ANOVA**:\r\n",
    "     - Estimates parameters through the posterior distributions, which give a full range of plausible values for each parameter and their probabilities.\r\n",
    "     - Posterior credible intervals (e.g., 95% credible intervals) provide ranges within which the parameter lies with a specific probability, given the observed data and prior information.\r\n",
    "     - These intervals are easier to interpret in terms of probability statements (e.g., there’s a 95% probability that the parameter lies within this interval, given the data and prior).\r\n",
    "\r\n",
    "### 3. **Hypothesis Testing**:\r\n",
    "   - **Classical ANOVA**:\r\n",
    "     - Tests hypotheses using p-values. A p-value less than a significance level (often 0.05) leads to rejection of the null hypothesis, indicating that at least one group mean is different.\r\n",
    "     - Relies on the F-test for hypothesis testing. However, it provides only an indirect measure of the evidence against the null hypothesis without direct probability statements about hypotheses.\r\n",
    "   \r\n",
    "   - **Bayesian ANOVA**:\r\n",
    "     - Hypothesis testing can be done by comparing posterior probabilities of models or by calculating **Bayes factors**, which quantify evidence for one model (e.g., that there is a difference among groups) over another (e.g., no difference among groups).\r\n",
    "     - Bayes factors provide a direct measure of the strength of evidence in favor of one hypothesis over another, allowing conclusions without relying on arbitrary significance levels (like 0.05).\r\n",
    "     - Offers a more flexible interpretation, as researchers can update their beliefs about the hypothesis directly based on the observed data and prior knowledge.\r\n",
    "\r\n",
    "### Summary of Key Differences:\r\n",
    "\r\n",
    "| Aspect               | Classical ANOVA                          | Bayesian ANOVA                              |\r\n",
    "|----------------------|------------------------------------------|---------------------------------------------|\r\n",
    "| **Uncertainty**      | p-values and confidence intervals        | Posterior distributions and credible intervals |\r\n",
    "| **Parameter Estimation** | Point estimates and confidence intervals | Full posterior distributions of parameters  |\r\n",
    "| **Hypothesis Testing**   | Based on p-values and F-statistic        | Bayes factors or posterior model probabilities |\r\n",
    "\r\n",
    "In essence, the Bayesian approach offers a more probabilistic, flexible framework for handling uncertainty, parameter estimation, and hypothesis testing. This flexibility comes with the requirement to specify prior information, which can significantly influence results, particularly with limited data. The frequentist approach, by contrast, is more straightforward and does not require priors but offers less flexibility in terms of probabilistic interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccaa4681-f6a4-4127-9c65-f729873459e3",
   "metadata": {},
   "source": [
    "### 8. Question: **You have two sets of data representing the incomes of two different professions1\n",
    "V Profession A: [48, 52, 55, 60, 62'\n",
    "V Profession B: [45, 50, 55, 52, 47] Perform an F-test to determine if the variances of the two professions'\n",
    "incomes are equal. What are your conclusions based on the F-test?\n",
    "\n",
    "Task: Use Python to calculate the F-statistic and p-value for the given data.\n",
    "\n",
    "Objective: Gain experience in performing F-tests and interpreting the results in terms of variance comparison.**\n",
    "\r\n",
    "Here’s the Python code used to perform the F-test:\r\n",
    "\r\n",
    "```python\r\n",
    "from scipy.stats import f\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "# Income data for the two professions\r\n",
    "profession_A = np.array([48, 52, 55, 60, 62])\r\n",
    "profession_B = np.array([45, 50, 55, 52, 47])\r\n",
    "\r\n",
    "# Calculate variances of both groups\r\n",
    "var_A = np.var(profession_A, ddof=1)  # Sample variance (ddof=1)\r\n",
    "var_B = np.var(profession_B, ddof=1)\r\n",
    "\r\n",
    "# Calculate the F-statistic\r\n",
    "F_statistic = var_A / var_B\r\n",
    "\r\n",
    "# Degrees of freedom for both samples\r\n",
    "df_A = len(profession_A) - 1\r\n",
    "df_B = len(profession_B) - 1\r\n",
    "\r\n",
    "# Calculate the p-value for the F-test\r\n",
    "p_value = 2 * min(f.cdf(F_statistic, df_A, df_B), 1 - f.cdf(F_statistic, df_A, df_B))\r\n",
    "\r\n",
    "F_statistic, p_value\r\n",
    "```\r\n",
    "\r\n",
    "### Results:\r\n",
    "- **F-statistic**: 2.09\r\n",
    "- **p-value**: 0.493\r\n",
    "\r\n",
    "### Interpretation:\r\n",
    "Since the p-value (0.493) is greater than the common significance level of 0.05, we fail to reject the null hypothesis. This suggests there is no statistically significant difference between the variances of incomes for Profession A and Profession B, indicating that the variances are likely equal.st."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4238654f-a461-4076-99d0-1ec8e04671a3",
   "metadata": {},
   "source": [
    "### Question 9: Conduct a one-way ANOVA to test whether there are any statistically significant differences in average heights between three different regions with the following data1\n",
    "V Region A: [160, 162, 165, 158, 164'\n",
    "\n",
    "V Region B: [172, 175, 170, 168, 174'\n",
    "\n",
    "V Region C: [180, 182, 179, 185, 183'\n",
    "\n",
    "V Task: Write Python code to perform the one-way ANOVA and interpret the results\f",
    "\n",
    "V Objective: Learn how to perform one-way ANOVA using Python and interpret F-statistic and p-value.\n",
    "\n",
    "### Here’s the Python code used to perform the one-way ANOVA:\r\n",
    "\r\n",
    "```python\r\n",
    "from scipy.stats import f_oneway\r\n",
    "\r\n",
    "# Height data for the three regions\r\n",
    "region_A = [160, 162, 165, 158, 164]\r\n",
    "region_B = [172, 175, 170, 168, 174]\r\n",
    "region_C = [180, 182, 179, 185, 183]\r\n",
    "\r\n",
    "# Perform one-way ANOVA\r\n",
    "F_statistic, p_value = f_oneway(region_A, region_B, region_C)\r\n",
    "\r\n",
    "F_statistic, p_value\r\n",
    "```\r\n",
    "\r\n",
    "### Results:\r\n",
    "- **F-statistic**: 67.87\r\n",
    "- **p-value**: \\(2.87 \\times 10^{-7}\\)\r\n",
    "\r\n",
    "### Interpretation:\r\n",
    "The very low p-value (close to 0) is much smaller than the typical significance level of 0.05. This indicates that we reject the null hypothesis and conclude that there are statistically significant differences in average heights between at least two of the regions. \r\n",
    "\r\n",
    "To determine which specific regions differ, further post-hoc tests (like Tukey’s HSD) would be necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e33cd6-0665-4bcc-b087-72297e3d2862",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
